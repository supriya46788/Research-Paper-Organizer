<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Paper Organizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #dbeafe 0%, #e0e7ff 100%);
            min-height: 100vh;
            color: #1f2937;
        }

        /* Navigation */
        nav {
            background: white;
            padding: 1rem 2rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .nav-links {
            display: flex;
            gap: 2rem;
            align-items: center;
        }

        .nav-links a {
            color: #6b7280;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }

        .nav-links a:hover {
            color: #3b82f6;
        }

        .btn {
            padding: 0.6rem 1.5rem;
            border: none;
            border-radius: 8px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
        }

        .btn-primary {
            background: #3b82f6;
            color: white;
        }

        .btn-primary:hover {
            background: #2563eb;
            transform: translateY(-1px);
        }

        .btn-secondary {
            background: #f3f4f6;
            color: #374151;
            border: 1px solid #d1d5db;
        }

        .btn-secondary:hover {
            background: #e5e7eb;
        }

        /* Main Container */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }

        /* Header Section */
        .header {
            background: white;
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .header-left h1 {
            font-size: 2rem;
            font-weight: bold;
            color: #1f2937;
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .header-left h1 i {
            color: #3b82f6;
        }

        .header-left p {
            color: #6b7280;
            font-size: 1.1rem;
        }

        .header-buttons {
            display: flex;
            gap: 1rem;
        }

        /* Search Section */
        .search-section {
            background: white;
            border-radius: 16px;
            padding: 1.5rem;
            margin-bottom: 2rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
        }

        .search-container {
            display: flex;
            gap: 1rem;
            align-items: center;
        }

        .search-input {
            flex: 1;
            padding: 0.75rem 1rem;
            border: 2px solid #e5e7eb;
            border-radius: 8px;
            font-size: 1rem;
        }

        .search-input:focus {
            outline: none;
            border-color: #3b82f6;
        }

        select {
            padding: 0.75rem 1rem;
            border: 2px solid #e5e7eb;
            border-radius: 8px;
            font-size: 1rem;
            background: white;
        }

        /* Main Content */
        .main-content {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
        }

        .papers-section {
            background: white;
            border-radius: 16px;
            padding: 2rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
        }

        .papers-section h2 {
            font-size: 1.5rem;
            margin-bottom: 1.5rem;
            color: #1f2937;
        }

        .paper-card {
            background: #3b82f6;
            color: white;
            padding: 1.5rem;
            border-radius: 12px;
            margin-bottom: 1rem;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .paper-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 16px rgba(59, 130, 246, 0.3);
        }

        .paper-card h3 {
            font-size: 1.2rem;
            margin-bottom: 0.5rem;
        }

        .paper-card .authors {
            font-size: 0.9rem;
            opacity: 0.8;
            margin-bottom: 1rem;
        }

        .paper-tags {
            display: flex;
            gap: 0.5rem;
            flex-wrap: wrap;
        }

        .tag {
            background: rgba(255, 255, 255, 0.2);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
        }

        .details-section {
            background: white;
            border-radius: 16px;
            padding: 2rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
            display: flex;
            align-items: center;
            justify-content: center;
            text-align: center;
        }

        .empty-state {
            color: #6b7280;
        }

        .empty-state i {
            font-size: 3rem;
            margin-bottom: 1rem;
            color: #d1d5db;
        }

        @media (max-width: 768px) {
            .main-content {
                grid-template-columns: 1fr;
            }
            
            .header {
                flex-direction: column;
                gap: 1rem;
                text-align: center;
            }
            
            .search-container {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav>
        <div class="nav-container">
            <div class="nav-links">
                <a href="#" class="btn btn-primary">Home</a>
                <a href="about.html">About us</a>
                <a href="Faq.html">FAQ</a>
                <a href="contact.html">Contact Us</a>
            </div>
        </div>
    </nav>

    <!-- Main Container -->
    <div class="container">
        <!-- Header Section -->
        <div class="header">
            <div class="header-left">
                <h1><i class="fas fa-book-open"></i> Research Paper Organizer</h1>
                <p>Organize, track, and cite your research papers efficiently</p>
            </div>
            <div class="header-buttons">
                <button class="btn btn-primary" onclick="addPaper()">
                    <i class="fas fa-plus"></i> Add Paper
                </button>
                <button class="btn btn-primary" onclick="openSummarizer()">
                    <i class="fas fa-brain"></i> Summarize Paper
                </button>
            </div>
        </div>

        <!-- Search Section -->
        <div class="search-section">
            <div class="search-container">
                <input type="text" class="search-input" placeholder="Search papers by title, author, or abstract...">
                <select>
                    <option>All Topics</option>
                    <option>Machine Learning</option>
                    <option>Data Science</option>
                </select>
                <select>
                    <option>From Year</option>
                </select>
                <select>
                    <option>To Year</option>
                </select>
                <input type="text" placeholder="Filter by Author" style="flex: 0.5;">
            </div>
        </div>

        <!-- Main Content -->
        <div class="main-content">
            <!-- Papers List -->
            <div class="papers-section">
                <h2>Papers (2)</h2>
                
                <div class="paper-card" onclick="selectPaper('attention')">
                    <h3>Attention Is All You Need</h3>
                    <p class="authors">Vaswani, A., Shazeer, N., Parmar, N., et al. • 2017 • NIPS</p>
                    <div class="paper-tags">
                        <span class="tag">Machine Learning</span>
                        <span class="tag"># NLP</span>
                        <span class="tag">#transformers</span>
                        <span class="tag">#attention</span>
                        <span class="tag">#nlp</span>
                    </div>
                </div>

                <div class="paper-card" onclick="selectPaper('bert')">
                    <h3>BERT: Pre-training of Deep Bidirectional Transformers</h3>
                    <p class="authors">Devlin, J., Chang, M. W., Lee, K., Toutanova, K. • 2018 • NAACL</p>
                    <div class="paper-tags">
                        <span class="tag">Machine Learning</span>
                        <span class="tag"># NLP</span>
                        <span class="tag">#bert</span>
                        <span class="tag">#pre-training</span>
                        <span class="tag">#bidirectional</span>
                    </div>
                </div>
            </div>

            <!-- Details Section -->
            <div class="details-section">
                <div class="empty-state">
                    <i class="fas fa-book-open"></i>
                    <p>Select a paper to view details</p>
                </div>
            </div>
        </div>
    </div>

    <script>
        // Sample paper data
        const papers = {
            attention: {
                title: "Attention Is All You Need",
                authors: "Vaswani, A., Shazeer, N., Parmar, N., et al.",
                year: 2017,
                journal: "NIPS",
                abstract: "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms.",
                notes: "Introduced the Transformer architecture that revolutionized NLP. Key insight: self-attention mechanism can replace recurrence and convolution."
            },
            bert: {
                title: "BERT: Pre-training of Deep Bidirectional Transformers",
                authors: "Devlin, J., Chang, M. W., Lee, K., Toutanova, K.",
                year: 2018,
                journal: "NAACL",
                abstract: "We introduce BERT, which stands for Bidirectional Encoder Representations from Transformers. BERT is designed to pre-train deep bidirectional representations.",
                notes: "BERT showed that bidirectional pre-training is crucial for language understanding tasks."
            }
        };

        let selectedPaper = null;

        function selectPaper(paperId) {
            selectedPaper = papers[paperId];
            console.log('Selected paper:', selectedPaper);
            
            // Update details section
            const detailsSection = document.querySelector('.details-section');
            detailsSection.innerHTML = `
                <div style="width: 100%; text-align: left;">
                    <h3 style="color: #3b82f6; margin-bottom: 1rem;">${selectedPaper.title}</h3>
                    <p style="color: #6b7280; margin-bottom: 1rem;"><strong>Authors:</strong> ${selectedPaper.authors}</p>
                    <p style="color: #6b7280; margin-bottom: 1rem;"><strong>Year:</strong> ${selectedPaper.year} | <strong>Journal:</strong> ${selectedPaper.journal}</p>
                    <p style="margin-bottom: 1rem;"><strong>Abstract:</strong></p>
                    <p style="color: #4b5563; margin-bottom: 1rem; line-height: 1.6;">${selectedPaper.abstract}</p>
                    <p style="margin-bottom: 0.5rem;"><strong>Notes:</strong></p>
                    <p style="color: #4b5563; line-height: 1.6;">${selectedPaper.notes}</p>
                </div>
            `;
        }

        function addPaper() {
            Swal.fire({
                title: 'Add New Paper',
                text: 'This would open the add paper form',
                icon: 'info',
                confirmButtonColor: '#3b82f6'
            });
        }

        async function openSummarizer() {
            // Check if a paper is selected
            if (!selectedPaper) {
                const result = await Swal.fire({
                    title: 'No Paper Selected',
                    text: 'Would you like to upload a PDF to summarize, or select a paper from the list first?',
                    icon: 'question',
                    showCancelButton: true,
                    confirmButtonText: 'Upload PDF',
                    cancelButtonText: 'Select Paper First',
                    confirmButtonColor: '#3b82f6',
                    cancelButtonColor: '#6b7280'
                });

                if (result.isConfirmed) {
                    // Redirect to PDF upload page
                    window.location.href = 'summarize.html';
                } else {
                    // Show message to select a paper
                    Swal.fire({
                        title: 'Select a Paper',
                        text: 'Please click on a paper from the list to select it, then try the Summarize button again.',
                        icon: 'info',
                        confirmButtonColor: '#3b82f6'
                    });
                }
                return;
            }

            // If paper is selected, show summarization options
            const { value: summaryType } = await Swal.fire({
                title: 'Summarize Selected Paper',
                text: `"${selectedPaper.title}"`,
                input: 'select',
                inputOptions: {
                    'brief': 'Brief Summary',
                    'detailed': 'Detailed Summary',
                    'methodology': 'Methodology Focus',
                    'findings': 'Key Findings'
                },
                inputPlaceholder: 'Select summary type',
                showCancelButton: true,
                confirmButtonColor: '#3b82f6',
                inputValidator: (value) => {
                    if (!value) {
                        return 'Please select a summary type!';
                    }
                }
            });

            if (summaryType) {
                // Show loading
                Swal.fire({
                    title: 'Generating Summary...',
                    text: 'AI is analyzing the paper content',
                    allowOutsideClick: false,
                    didOpen: () => {
                        Swal.showLoading();
                    }
                });

                // Simulate API call
                setTimeout(() => {
                    generateSummary(summaryType);
                }, 2000);
            }
        }

        function generateSummary(type) {
            // Mock summary generation based on selected paper and type
            let summary = '';
            
            if (selectedPaper.title.includes('Attention')) {
                switch(type) {
                    case 'brief':
                        summary = 'The Transformer architecture replaces recurrence and convolution with self-attention mechanisms, achieving state-of-the-art results in machine translation tasks while being more parallelizable and requiring less time to train.';
                        break;
                    case 'detailed':
                        summary = `This groundbreaking paper introduces the Transformer architecture, a novel approach to sequence modeling that relies entirely on attention mechanisms. The model eliminates the need for recurrent and convolutional layers, instead using multi-head self-attention to capture dependencies between input and output sequences.

Key innovations include:
• Multi-head attention mechanism that allows the model to attend to different representation subspaces
• Positional encoding to inject sequence order information
• Layer normalization and residual connections for training stability

The Transformer achieves superior performance on machine translation benchmarks while being significantly more efficient to train due to its parallelizable architecture.`;
                        break;
                    case 'methodology':
                        summary = 'The methodology centers on the self-attention mechanism, specifically multi-head attention, which computes attention weights for all positions in a sequence simultaneously. The architecture uses encoder-decoder structure with 6 layers each, employing scaled dot-product attention and position-wise feed-forward networks.';
                        break;
                    case 'findings':
                        summary = 'Key findings demonstrate that attention mechanisms alone are sufficient for state-of-the-art sequence modeling, achieving BLEU scores of 28.4 on WMT 2014 English-to-German translation. The model trains significantly faster than recurrent architectures while maintaining superior performance.';
                        break;
                }
            } else if (selectedPaper.title.includes('BERT')) {
                switch(type) {
                    case 'brief':
                        summary = 'BERT introduces bidirectional pre-training for language representations, achieving significant improvements on eleven natural language processing tasks by pre-training deep bidirectional representations from unlabeled text.';
                        break;
                    case 'detailed':
                        summary = `BERT (Bidirectional Encoder Representations from Transformers) presents a new method for pre-training language representations that obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.

Key contributions:
• Demonstrates the importance of bidirectional pre-training for language representations
• Introduces masked language model (MLM) pre-training objective
• Shows that pre-trained representations can eliminate the need for task-specific architectures
• Achieves new state-of-the-art results on eleven NLP tasks including GLUE, SQuAD, and others

The bidirectional training allows BERT to learn deeper bidirectional representations by jointly conditioning on both left and right context in all layers.`;
                        break;
                    case 'methodology':
                        summary = 'BERT uses a masked language model (MLM) pre-training objective, randomly masking 15% of input tokens and predicting them. It also employs next sentence prediction (NSP) to understand relationships between sentences. The model is based on the Transformer encoder architecture.';
                        break;
                    case 'findings':
                        summary = 'BERT achieves substantial improvements across tasks: 7.7% point absolute improvement on GLUE, 1.5 F1 improvement on SQuAD v1.1, and 5.1 F1 improvement on SQuAD v2.0. The bidirectional training proves crucial for these performance gains.';
                        break;
                }
            }

            // Show the summary result
            Swal.fire({
                title: `${type.charAt(0).toUpperCase() + type.slice(1)} Summary`,
                html: `<div style="text-align: left; max-height: 400px; overflow-y: auto;">
                    <h4 style="color: #3b82f6; margin-bottom: 10px;">${selectedPaper.title}</h4>
                    <p style="line-height: 1.6; white-space: pre-line;">${summary}</p>
                </div>`,
                width: 600,
                confirmButtonText: 'Copy Summary',
                showCancelButton: true,
                cancelButtonText: 'Close',
                confirmButtonColor: '#3b82f6'
            }).then((result) => {
                if (result.isConfirmed) {
                    navigator.clipboard.writeText(summary).then(() => {
                        Swal.fire({
                            title: 'Copied!',
                            text: 'Summary has been copied to clipboard',
                            icon: 'success',
                            timer: 1500,
                            showConfirmButton: false
                        });
                    });
                }
            });
        }

        // Add some visual feedback for button clicks
        document.addEventListener('DOMContentLoaded', function() {
            const buttons = document.querySelectorAll('.btn');
            buttons.forEach(button => {
                button.addEventListener('click', function() {
                    this.style.transform = 'scale(0.98)';
                    setTimeout(() => {
                        this.style.transform = '';
                    }, 100);
                });
            });
        });
    </script>
</body>
</html>
